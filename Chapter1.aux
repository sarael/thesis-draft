\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Dataset}{1}}
\newlabel{section:dataset}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Dataset characteristics}{1}}
\citation{nltk:2006,nltk:2009}
\citation{nltk:2006,nltk:2009}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Dataset statistics\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{datastat}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Problem statement}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Basic notations}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Vocabulary}{2}}
\newlabel{vocab}{{2}{2}}
\newlabel{subsection:text preprocessing}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Overview of the system\relax }}{3}}
\newlabel{introChapter1}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Different dataset entities\relax }}{3}}
\newlabel{umlDiagram}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Distribution of the number  of ratings per movie\relax }}{4}}
\newlabel{NbRatingsPerMovie}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Distribution of the number   of reviews per movie\relax }}{4}}
\newlabel{NbReviewsPerMovie}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution of the number  of ratings per user \relax }}{4}}
\newlabel{NbRatingsPerUser}{{5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Distribution of the number  of reviews per user\relax }}{4}}
\newlabel{NbReviewsPerUser}{{6}{4}}
\citation{harris:1954}
\citation{church:1989}
\citation{church:1989}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Distribution of the number of words per review\relax }}{5}}
\newlabel{NbWordsPerReview}{{7}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces CDF of the number of words per sentence\relax }}{5}}
\newlabel{NbOfWordsPerSentences}{{8}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Words representation}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Background}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Word Count/context matrix}{5}}
\citation{turney:2010}
\citation{svd:1936}
\citation{lsa:1990}
\citation{turney:2001}
\citation{glove:2014}
\citation{mikolov:2013}
\citation{fyshe:2015}
\citation{levy:2015,mikolov:2013}
\citation{collobert:2011,fyshe:2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Words representations from prediction}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Word2vec}{6}}
\citation{mikolov:2013}
\citation{mikolov:2013}
\citation{mikolov:2013}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  The CBOW model predicts the current word based on the context (image on the left), and the Skip-gram predicts surrounding words given the current word (image on the right). \relax }}{7}}
\newlabel{word2vecmodels}{{9}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Word2vec models}{7}}
\citation{mikolov:2013}
\citation{mikolov:2013}
\citation{mikolov:2013}
\citation{mikolov:2013}
\citation{mikolov:2013}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}The Skip gram model}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Word2Vec Parameters Setting}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Dynamic window size}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Sub-Sampling}{8}}
\citation{tsne:2008}
\citation{tsne:2008}
\citation{pca:1987}
\citation{svd:1936}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Clusters of semantically similar words emerge when the word2vec vectors are projected down to 2D using t-SNE. \relax }}{9}}
\newlabel{w2vecclusters}{{10}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Rare words delation}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Word2vec examples}{9}}
\citation{manning:1999}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Examples of the closest words to a given word using the Word2Vec model.\relax }}{10}}
\newlabel{wordvec}{{2}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Semantic similarity}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Similarity Metrics on Vectors}{10}}
\citation{manning:1999}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Analogy task (main-actor:movie) based on word2vec vectors \relax }}{11}}
\newlabel{analogytask}{{11}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Similarity Metrics on Vector Sets}{11}}
\citation{wmd:2015}
\citation{emd:1998}
\citation{emdcomlexity:2009}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Closest pairs distance}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Word movers distance}{12}}
\bibstyle{abbrv}
\bibdata{Bibliography}
\bibcite{nltk:2006}{{1}{}{{}}{{}}}
\bibcite{nltk:2009}{{2}{}{{}}{{}}}
\bibcite{church:1989}{{3}{}{{}}{{}}}
\bibcite{collobert:2011}{{4}{}{{}}{{}}}
\bibcite{lsa:1990}{{5}{}{{}}{{}}}
\bibcite{svd:1936}{{6}{}{{}}{{}}}
\bibcite{fyshe:2015}{{7}{}{{}}{{}}}
\bibcite{harris:1954}{{8}{}{{}}{{}}}
\bibcite{wmd:2015}{{9}{}{{}}{{}}}
\bibcite{levy:2015}{{10}{}{{}}{{}}}
\bibcite{tsne:2008}{{11}{}{{}}{{}}}
\bibcite{manning:1999}{{12}{}{{}}{{}}}
\bibcite{mikolov:2013}{{13}{}{{}}{{}}}
\bibcite{emdcomlexity:2009}{{14}{}{{}}{{}}}
\bibcite{glove:2014}{{15}{}{{}}{{}}}
\bibcite{emd:1998}{{16}{}{{}}{{}}}
\bibcite{turney:2001}{{17}{}{{}}{{}}}
\bibcite{turney:2010}{{18}{}{{}}{{}}}
\bibcite{pca:1987}{{19}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
